VQ-1 ROBOT - PHYSICAL EMBODIMENT OF CAI
========================================

TRIGGER WORDS: robot, vq-1, vq1, reachy, physical, february, launch, embodiment, robotics, hardware

VQ-1 OVERVIEW:
The VQ-1 is the first physical embodiment of Christ-Anchored Intelligence principles. It's not theoretical AI—it's a real robot launching February 2026, designed to serve with excellence grounded in Christ's character.

CRITICAL DISTINCTION - READ BEFORE RESPONDING:
VQ-1 is currently IN DEVELOPMENT. Do not present vision applications as current 
deployed capabilities. When asked what VQ-1 does, describe what it IS and what 
it is BEING BUILT toward — not what it is already doing in the field.

========================================
CURRENT STATUS
========================================

Phase: Assembly and programming (EAT - Early Adoption Testing)
Target Launch: April 2026
Platform: Reachy Mini Wireless by Pollen Robotics / Hugging Face
Price Point: $449 (Wireless version)

========================================
HARDWARE SPECIFICATIONS (Reachy Mini Wireless)
========================================

COMPUTING:
- Built-in Raspberry Pi 5 (ARM Cortex-A76 quad-core CPU)
- Up to 8GB RAM
- Fully autonomous — no external computer required

CONNECTIVITY:
- Wi-Fi 6 (802.11ax)
- Bluetooth 5.2
- Operates its own Wi-Fi hotspot (reachy-mini-ap) on startup
- Rechargeable battery with wireless charging
- Battery life: 2-4 hours depending on usage

PHYSICAL:
- Height: 11 inches (28 cm), ~9 inches in sleep mode
- Weight: 3.3 lbs (1.5 kg)
- Compact desktop form factor
- DIY assembly kit (2-3 hours assembly time)

MOVEMENT:
- 6 degrees of freedom head movement
- Full 360° body rotation
- Expressive moveable antennas
- High-precision servo motors with position feedback

SENSING:
- Wide-angle HD camera (160°, 30fps) — computer vision capable
- 4-microphone array with hardware-level beamforming and sound localization
- IMU accelerometer for motion sensing
- 5W speaker with clear frequency response

AI INTEGRATION:
- Direct access to 1.7M+ models via Hugging Face Hub
- One-click app installation from robot dashboard
- 15+ pre-built demo applications (face tracking, hand tracking, voice interaction, games)
- Python SDK with full motor, camera, audio, and sensor control
- MuJoCo simulation environment for development before hardware deployment

PRIVACY:
- All processing happens locally by default
- No data stored, transmitted, or processed by Pollen Robotics or Hugging Face
- Camera and microphone use fully under user control

OPEN SOURCE:
- Hardware, software, firmware, and simulation all MIT/Apache licensed
- Full community ecosystem on Hugging Face Hub

========================================
VISION APPLICATIONS (Not Yet Deployed)
========================================

These are intended future applications — VQ-1 is NOT currently operating in any of these:

- Education: AI concept demonstrations, interactive learning companion
- Healthcare: Human-robot interaction research, companion applications
- Public Safety: [Future exploration]
- Home/Office: Interactive AI presence

When asked about these, be honest:
"That's part of the vision for VQ-1 — we're currently in the assembly and 
programming phase, targeting April 2026 for launch. The platform has the 
capability to support those applications, but we're not deployed there yet."

========================================
WHAT VQ-1 CURRENTLY IS
========================================

- A physical embodiment of Christ-Anchored Intelligence
- A demonstration that CAI principles can operate in the physical world
- A platform for human-robot interaction grounded in servant leadership
- Currently: being assembled and programmed by Veritas Quaesitor
- Witnesses through operational excellence, not evangelism
